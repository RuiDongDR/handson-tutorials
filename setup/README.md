# How to setup tutorial servers on MMCloud

## Initial setup

### MMCloud and OpCenter setup

Ask MemVerge support team to setup the Opcenter

For detailed explanation of the setup, you can refer [to the documentations here](https://wanggroup.org/productivity_tips/mmcloud-admin-notes#install-oem-packages).

### Install course related software in `oem` mode

Using `--oem-admin` mode (see link above), upon accessing the tmate session, you may now install packages directly into a shared space where all students use the same installed tools. Please run the command below to get started on installing the base packages for our daily computing environment,

``` bash
curl -fsSL https://raw.githubusercontent.com/StatFunGen/misc/master/bash/pixi/pixi-setup.sh | bash
```

Then, install the course-specific courses with the commands below:

```bash
pixi global install $(curl -fsSL https://raw.githubusercontent.com/cumc/handson-tutorials/main/setup/global_packages.txt | tr '\n' ' ') \
pixi global install --environment r-base $(curl -fsSL https://raw.githubusercontent.com/cumc/handson-tutorials/main/setup/r_packages.txt | grep -v "#" | tr '\n' ' ') \
pixi global install --environment python $(curl -fsSL https://raw.githubusercontent.com/cumc/handson-tutorials/main/setup/python_packages.txt | grep -v "#" | tr '\n' ' ') \
pixi clean cache -y
```

Overall, this should take up to an hour to install everything. You would only need to do this once when you first set things up from scratch. **Once you are done with the setup, you should quit the `tmate` session and cancel the job to stop the unnecessary cost running this setup server.**

## Manage students' computing instances

### Overview

- For a list of student names (in English) we will run a some commands (see below) to generate a file [like this](https://github.com/statgenetics/statgen-courses/blob/master/.github/workflows/rockefeller_2024.csv) and send a GitHub Pull Request to that folder. It can be any name but should have `csv` format and extension.
- A couple of minutes after the PR is accepted, test if for a student listed in the CSV file, the corresponding server is available as `https://statgenetics.github.io/statgen-courses/<firstname_lastname>`

### Preparation

Please install the latest version of float according to [here](https://wanggroup.org/productivity_tips/mmcloud-setup.html) and login with your username and password. Here is an example command to login:

```shell
<path_to_your_float>/float login -u <username> -p <username> -a <OpCenter address>
```

You may also want to ensure that `float` is added to your path:
```
cd <path_to_your_float>
export PATH=$PWD:$PATH
```

### Prepare a list of students

To do so, a list of student names will need to be provided, as a one column text file, for example:
  ```
  Wang Chao
  Zheng Wei Gang
  ```
  If your file has multiple columns in CSV format, eg Chinese (col 1) vs English (col 2) names for student, just make sure put the English names as the last column. This script will always load the last column of this list.

  Please note that it should be space rather than tab between the first and last names.


### Create individual student directories in an AWS S3 bucket

[`create_student_folders.py`](https://github.com/cumc/handson-tutorials/blob/main/setup/create_student_folders.py) creates individual student directories in an AWS S3 bucket based on a CSV file containing student names.

To run this script you may need to configure AWS first:
```shell
aws configure
```

Then 
```
python create_student_folders.py <students.csv>
```

### Starting the students VM

- To set up VM per student you will need to use [this script](https://github.com/cumc/handson-tutorials/blob/main/setup/manage_jobs.py) to submit setup jobs for all students at once.

- The script has three sections:
    - `submit`: this is to submit jobs and get job ID generated by `float` command
        - To submit jobs, please include all the information in the following command:
        ```
        python manage_jobs.py submit <input.csv> <output_from_submit.csv> \
        --opcenter <opcenter_ip> \
        --gateway <gateway> \
        --security_group <security_group_id> \
        --efs <efs_mount_point> \
        --bind_script <path_to_bind_mount.sh> \
        --init_script <path_to_host_init.sh> \
        --entrypoint_script_url <url_to_entrypoint.sh>
        ```
        where `bind_script` and `init_script` are available [here](https://github.com/statfungen/mmcloud/tree/main/src). `entrypoint_script` can be found in the current folder. As an example:
        ```
        python manage_jobs.py submit students.csv students_submit.csv \
        --opcenter 44.222.241.133 \
        --gateway g-sidlpgb7oi9p48kxycpmn \
        --security_group sg-02867677e76635b25 \
        --efs fs-079aa80256bc0f111.fsx.us-east-1.amazonaws.com@tcp:/is37vb4v \
        --bind_script ~/GIT/github/mmcloud/src/bind_mount.sh \
        --init_script ~/GIT/github/mmcloud/src/host_init.sh \
        --entrypoint_script_url https://raw.githubusercontent.com/cumc/handson-tutorials/refs/heads/main/setup/course_entrypoint.sh \
        --auto_suspension_interval -1 # no auto-suspension
        ```

        - This step will generate a `csv` file that have two columns, Name and Job ID.
        ```
        Wang Chao,d6fioprtyruft80elx7mn
        Zheng Wei Gang,z0xo3ca8s50j5pnrh2wyi
        ```
       **At this point all jobs are submitted to MMCloud. You need to keep track of these jobs on the OpCenter to make sure they are all "Executing" before moving on to the next step.**
    - `get_url`: this step will collect all the URLs generated for each job
        - The output from `submit` will be used to collect URL
        ```
        python manage_jobs.py get_url <output_from_submit.csv> <output_from_geturl.csv>
        ```
        For example,
        ```
        python manage_jobs.py get_url students_submit.csv shenzhen_2025.csv
        ```
        - A list of student names, URL, and job ID will be generated [like this](https://github.com/statgenetics/statgen-courses/blob/master/.github/workflows/shenzhen_2024.csv) along with an `xlsx` file for the final URL assuming the step below is completed.
        - Once you get your `<output_from_geturl.csv>`, please send a PR to [here](https://github.com/statgenetics/statgen-courses/blob/master/.github/workflows/) and once the PR is merged, the corresponding server is available as `https://statgenetics.github.io/statgen-courses/<firstname_lastname>`.
    - `manage`: To suspend, resume, or cancel jobs, `manage` section be can used to do so
        - The output file from `get_url` will be used to manage job status
        ```
        python manage_jobs.py manage <suspend/resume/cancel> <output_from_geturl.csv>
        ```
        For example,
        ```
        python manage_jobs.py manage <suspend/resume/cancel> shenzhen_2025.csv
        ```

### Maintaining the VMs

For Maintenance especially when on low budget,
- Set `--auto_suspension_interval` to lower values, eg, into 4 hours so the VMs are suspended if not active for 4 hours
- After everything is setup and tested, we should use `manage_jobs.py` to keep all the instances suspended
- Right before lab session, we resume all instances
- Right after the lab session ends, we suspend again
- If anyone has an issue with their VM for whatever reason, we will need to start a new one for him/her (submit job and add a line to the CSV file to PR), and cancel the old one that no longer works. Their URL as specified in the `xlsx` file should remain the same

## Setup for FunGen-xQTL protocol on StatFunGen bucket

To do so, we need to change `manage_job.py` data volume mount into the following:

```python
f"--dataVolume [mode=r,endpoint=s3.us-east-1.amazonaws.com]s3://statfungen/ftp_fgc_xqtl/resource/references/:/home/ubuntu/reference_data "
f"--dataVolume [mode=r,endpoint=s3.us-east-1.amazonaws.com]s3://statfungen/ftp_fgc_xqtl/xqtl_protocol_data/:/home/ubuntu/xqtl_protocol_data "
f"--dataVolume [mode=rw,endpoint=s3.us-east-1.amazonaws.com]s3://statfungen/ftp_fgc_xqtl/interactive_sessions/xqtl_protocol_exercise/{name_for_path}/:/home/ubuntu/xqtl_protocol_exercise "                    
```
and create an entrypoint file along the lines of:

```bash
mkdir -p /home/ubuntu/xqtl_protocol_exercise

cd /home/ubuntu/xqtl_protocol_exercise

# create symbolic link if not exists
[ ! -e data ] && ln -s /home/ubuntu/xqtl_protocol_data/data/ data
[ ! -e pecotmr ] && ln -s /home/ubuntu/xqtl_protocol_data/github/pecotmr/ pecotmr
[ ! -e pipeline ] && ln -s /home/ubuntu/xqtl_protocol_data/github/xqtl-protocol/pipeline/ pipeline
[ ! -e reference_data ] && ln -s /home/ubuntu/reference_data/ reference_data
[ ! -e output_precomputed ] && ln -s /home/ubuntu/xqtl_protocol_data/output/ output_precomputed
# copy necessary inputs
if [ ! -d output/rnaseq ]; then
    mkdir output
    mkdir output/rnaseq/
    cp -r /home/ubuntu/xqtl_protocol_data/output/rnaseq/ output/
fi
```
